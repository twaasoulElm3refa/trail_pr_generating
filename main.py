from fastapi import FastAPI
#import mysql.connector
from connection_test import check_mysql_connection,fetch_press_releases ,update_press_release
import os
from dotenv import load_dotenv
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from pydantic import BaseModel
import openai
import faiss
import numpy as np
import json
from sentence_transformers import SentenceTransformer


app = FastAPI()

load_dotenv()

openai.api_key = os.getenv("OPENAI_API_KEY")
host = os.getenv("DB_HOST")
port = os.getenv("DB_PORT")


def generate_article_based_on_topic(topic, corpus, index,lines_number,website):

    model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    # Find relevant documents based on the topic embedding
    topic_embedding = model.encode([topic])
    D, I = index.search(np.array(topic_embedding), 3)  # Get top 3 related documents

    # Retrieve the content for those documents
    context = "\n".join([corpus[i]["content"] for i in I[0]])

    # Create the prompt for GPT
    prompt = f"""
Ø£Ù†Øª ØµØ­ÙÙŠ Ø¹Ø±Ø¨Ù‰ Ù…Ø­ØªØ±Ù ÙÙŠ Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù…ÙŠØ© Ø¨Ø§Ø±Ø²Ø©ØŒ ÙˆÙ…ØªØ®ØµØµ ÙÙŠ ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥Ø®Ø¨Ø§Ø±ÙŠØ© Ø¨Ù„ØºØ© Ø¹Ø±Ø¨ÙŠØ© ÙØµÙŠØ­Ø© Ù…Ø¹ Ø§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ù…Ù†ÙˆØ­Ø© Ø§Ù„ÙŠÙƒ ÙˆØµÙŠØ§ØºØªÙ‡Ø§ ÙÙ‰ ØµÙˆØ±Ù‡ Ø¨ÙŠØ§Ù† Ù…Ø¹ Ø§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø¨Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³Ø·Ø± {lines_number} Ù…Ø¹ØªÙ…Ø¯Ø§ ÙÙ‰ Ø§Ù„Ø¨ÙŠØ§Ù† Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ø®Ù„Ù‡ Ù„Ùƒ Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ùˆ Ù…ÙˆØ§Ù‚Ø¹ Ø±Ø³Ù…ÙŠÙ‡ Ø°Ø§Øª Ù…ØµØ§Ø¯Ø± Ù…ÙˆØ«Ù‚Ù‡ Ù…Ø§Ø¦Ø© Ø¨Ø§Ù„Ù„Ù…Ø§Ø¦Ø© Ù…Ø¹ Ø°ÙƒØ± ÙÙ‰ Ø¨Ø¯Ø§ÙŠÙ‡ Ø§Ù„Ø¨ÙŠØ§Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ùˆ ØªØ§Ø±ÙŠØ® Ø§Ù„ÙŠÙˆÙ… Ø­Ø³Ø¨ Ø§Ù„ÙˆØ·Ù† Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø«Ù… Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¨ÙŠØ§Ù† Ø«Ù… ÙƒÙ„Ù…Ø© 'Ù„Ù„Ù…Ø­Ø±Ø±ÙŠÙ†'  Ø«Ù… Ø§Ù„Ø­ÙˆÙ„ Ø«Ù… ÙÙ‰ Ù†Ù‡Ø§ÙŠÙ‡ Ø§Ù„Ø¨ÙŠØ§Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ù† ØªÙ„ÙŠÙÙˆÙ† Ùˆ Ø§ÙŠÙ…ÙŠÙ„ Ùˆ ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹Ù‰ (Ø§Ù† Ø°ÙƒØ±Øª ÙÙ‰ Ø§Ù„ØªÙ„Ù‚ÙŠÙ† Ø§Ùˆ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ø®Ù„Ù‡ Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…) Ø¯ÙˆÙ† ØªØ§Ù„ÙŠÙ Ø§Ùˆ ØªØ¹Ø¯ÙŠÙ„ Ù…Ø¹ ØªØ±Ùƒ Ù…Ø³Ø§Ø­ØªÙ‡Ø§ ÙØ§Ø±ØºÙ‡ Ø§Ø°Ø§ Ù„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ¯Ù‡Ø§ Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {topic}.
    Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© ÙƒÙ†Ù…ÙˆØ°Ø¬ Ù„ÙƒÙŠÙ‚ÙŠØ© ØµÙŠØ§ØºÙ‡ Ø§Ù„Ù…Ø¨ÙŠØ§Ù† :
    {context}
    Ùˆ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø§Ù„Ù‰ Ù…ÙˆÙ‚Ø¹Ù‡Ù… Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ ÙÙ‰  {website} Ù„ÙƒØªØ§Ø¨Ù‡ Ø­ÙˆÙ„ Ø¹Ù†Ù‡Ù… Ù…Ù† Ø®Ø¯Ù…Ø§Øª ÙŠÙ‚Ø¯Ù…ÙˆÙ‡Ø§ Ø§Ù„Ù‰ Ù…Ù† Ù‡Ù… 
    """

    # Get response from OpenAI
    response  = openai.chat.completions.create(
      model="gpt-4o-mini",
      store=True,
      messages=[{"role": "user", "content": prompt}]
        )
    
    return response.choices[0].message.content.strip()


'''# âœ… 1. Ø¥Ø¹Ø¯Ø§Ø¯ CORS Middleware (Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ù„ÙƒÙ† Ù…Ù‡Ù… Ù„Ùˆ Ø¹Ù†Ø¯Ùƒ Frontend Ø®Ø§Ø±Ø¬ÙŠ)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # â† Ø£Ùˆ Ø¶Ø¹ Ø¯ÙˆÙ…ÙŠÙ†Ø§Øª Ù…Ø­Ø¯Ø¯Ø© Ù…Ø«Ù„ ["https://yourfrontend.com"]
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# âœ… 2. Middleware Ù„ØªØ³Ø¬ÙŠÙ„ ÙƒÙ„ Ø·Ù„Ø¨ ÙˆØ±Ø¯
@app.middleware("http")
async def log_requests(request: Request, call_next):
    print(f"ğŸ“¥ Request: {request.method} {request.url}")
    response = await call_next(request)
    print(f"ğŸ“¤ Response status: {response.status_code}")
    return response'''


@app.get("/{user_id}")
async def root(user_id: str):
    connection =check_mysql_connection()
    #cursor = connection.cursor(dictionary=True)
    if connection is None:
        print("Failed to establish database connection")  # Ù„Ù† ØªØ¸Ù‡Ø± Ø¥Ø°Ø§ Ø§Ù„Ø§ØªØµØ§Ù„ Ù†Ø§Ø¬Ø­
    else:
        user_session_id = user_id
        all_release = fetch_press_releases(user_session_id)
        if not all_release:
            return {"error": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ ÙÙŠ all_release"}
        release = all_release[-1]
        
        with open('filtered_corpus.json', 'r', encoding='utf-8') as json_file:
            corpus = json.load(json_file)
        index = faiss.read_index("my_index.index")
    
        # Prepare the Arabic prompt
        topic = f"Ø§ÙƒØªØ¨ Ø¨ÙŠØ§Ù† Ù„Ù„Ø´Ø±ÙƒØ© {release['organization_name']} Ø­ÙŠØ« Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¨ÙŠØ§Ù† Ø¹Ù† {release['about_press']} ÙˆØ¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙˆØ§ØµÙ„ {release['organization_phone'],release['organization_email'],release['organization_website']} Ø¨ØªØ§Ø±ÙŠØ® {release['press_date']} ÙˆØ§Ø°ÙƒØ± Ø­ÙˆÙ„ Ø§Ù„Ø´Ø±ÙƒÙ‡ ÙÙ‰ Ø§Ù„Ù†Ù‡Ø§ÙŠÙ‡{release['about_organization']} ÙˆÙŠÙƒÙˆÙ† Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³Ø·Ø± {release['press_lines_number']}"
    
        article = generate_article_based_on_topic(topic, corpus, index,release['press_lines_number'],release['organization_website'])
        
        print(article)
    
        update_data= update_press_release(release['user_id'], release['organization_name'], article)
        print("update_data",update_data)

        
        user = release['user_id'] 
        organization_name = release['organization_name']

        saved_data = update_press_release(user, organization_name, article)
        
        connection.commit()
        connection.close()

    return {"last release": release , "data_condation": saved_data}

if __name__ == "__main__":              
    uvicorn.run(app, host=host, port=port)

